<?xml version="1.0" encoding="UTF-8" ?>
<schema name="smartSearch" version="1.5">
  <fields>

    <field name="id" type="string" indexed="true" stored="true" required="true" multiValued="false" />
    <field name="_version_" type="long" indexed="true" stored="true"/>
    <field name="timestamp" type="date" indexed="true" stored="true" default="NOW" multiValued="false"/>

    <!-- (deprecated) german stemmed fields (term vectors on for more like this)
         these are replaced by language mapped dynamic fields *_de, *_en

         Will be removed in future releases
         -->
    <field name="dbsearch_content_t" type="text_de" indexed="true" stored="false" termVectors="true" />
    <field name="dbsearch_hidden_content_t" type="text_de" indexed="true" stored="false" termVectors="true" />
    <field name="dbsearch_title_t" type="text_de" indexed="true" stored="false" termVectors="true" />
    <field name="dbsearch_extracted_title_t" type="text_de" indexed="true" stored="false" termVectors="true" />

    <!-- field for searching authors (term vectors on for more like this) -->
    <field name="dbsearch_author_t"           type="string" indexed="true" stored="true" termVectors="true" />
    <field name="dbsearch_extracted_author_t" type="string" indexed="true" stored="true" termVectors="true" />

    <!-- field with analyzed author data -->
    <field name="dbsearch_author_analyzed_t"           type="text_general" indexed="true" stored="true" termVectors="true" />
    <copyField source="dbsearch_author_t"           dest="dbsearch_author_analyzed_t" />

    <!-- field for facetting authors untokenized -->
    <field name="dbsearch_author_ss" type="string" indexed="true" stored="false" multiValued="true" />
    <copyField source="dbsearch_author_t"           dest="dbsearch_author_ss" />

    <!-- field for searching keywords (term vectors on for more like this) -->
    <field name="dbsearch_keywords_txt" type="text_general" indexed="true" stored="true" multiValued="true" termVectors="true" />

    <!-- field for facetting keywords untokenized -->
    <field name="dbsearch_keywords_ss" type="string" indexed="true" stored="false" multiValued="true" />
    <copyField source="dbsearch_keywords_txt" dest="dbsearch_keywords_ss" />

    <!-- field for searching taxonomies (term vectors on for more like this) -->
    <field name="dbsearch_taxonomies_txt" type="text_general" indexed="true" stored="false" multiValued="true" termVectors="true" />

    <!-- field for highlighting (first 25000 chars copied from content in update processor) -->
    <field name="dbsearch_highlight_t_de" type="text_de" indexed="true" stored="true" termVectors="true" termPositions="true" termOffsets="true" />
    <field name="dbsearch_highlight_t_en" type="text_en" indexed="true" stored="true" termVectors="true" termPositions="true" termOffsets="true" />
    <!-- depcrecated, will be removed in future releases -->
    <field name="dbsearch_highlight_t"    type="text_de" indexed="true" stored="true" termVectors="true" termPositions="true" termOffsets="true" />

    <!-- field for storing an excerpt (first 200 chars copied from content in update processor) -->
    <field name="dbsearch_excerpt_s" type="string" indexed="false" stored="true" />

    <!-- field for discriminating general documents from media documents -->
    <field name="dbsearch_doctype_s" type="string" indexed="true" stored="true" default="Document" />

    <field name="dbsearch_thumbnail_data" type="binary" indexed="false" stored="true" />
    <field name="dbsearch_image_url"    type="string"   indexed="false" stored="true" />
    <field name="dbsearch_image_link"   type="string"   indexed="false" stored="true" />
    <field name="dbsearch_image_format" type="string"   indexed="true"  stored="true" />
    <field name="dbsearch_image_width"  type="int"      indexed="true"  stored="true"/>
    <field name="dbsearch_image_height" type="int"      indexed="true"  stored="true"/>
    <field name="dbsearch_image_depth"  type="int"      indexed="true"  stored="true" />
    <field name="dbsearch_image_size"   type="int"      indexed="true"  stored="true" />

    <!--
         field for the spellchecker and autocompletion via facet.prefix.
         The name is deprecated because the field is no longer considered to be language specific
    -->
    <field name="dbsearch_suggest_de" type="textSuggest" indexed="true" stored="false" multiValued="true" />
    <copyField source="dbsearch_content_t_*"        dest="dbsearch_suggest_de" />
    <copyField source="dbsearch_title_s"            dest="dbsearch_suggest_de" />
    <copyField source="dbsearch_abstract_t_*"       dest="dbsearch_suggest_de" />
    <copyField source="dbsearch_extracted_title_s"  dest="dbsearch_suggest_de" />
    <copyField source="dbsearch_author_t"           dest="dbsearch_suggest_de" />
    <copyField source="dbsearch_extracted_author_t" dest="dbsearch_suggest_de" />
    <copyField source="dbsearch_keywords_txt"       dest="dbsearch_suggest_de" />
    <copyField source="dbsearch_taxonomies_txt"     dest="dbsearch_suggest_de" />

    <field name="dbsearch_title_s_sort" type="alphaNumericSort" indexed="true" stored="false" required="false" multiValued="false" />
    <copyField source="dbsearch_title_s" dest="dbsearch_title_s_sort" />

    <!-- Timebased document visibility -->
    <field name="dbsearch_valid_from_date_tdt" type="tdate" indexed="true" stored="true" default="0000-01-01T00:00:00.000Z" />
    <field name="dbsearch_valid_to_date_tdt"   type="tdate" indexed="true" stored="true" default="9999-12-31T23:59:59.999Z"/>

    <!-- authorization fields -->
    <field name="allow_token_document" type="string" indexed="true" stored="false" multiValued="true" required="false" default="__nosecurity__"/>
    <field name="allow_token_share" type="string" indexed="true" stored="false" multiValued="true" required="false" default="__nosecurity__"/>
    <field name="deny_token_document" type="string" indexed="true" stored="false" multiValued="true" required="false" default="__nosecurity__"/>
    <field name="deny_token_share" type="string" indexed="true" stored="false" multiValued="true" required="false" default="__nosecurity__"/>
    <field name="allow_token_parent" type="string" indexed="true" stored="false" multiValued="true" required="false" default="__nosecurity__"/>
    <field name="deny_token_parent" type="string" indexed="true" stored="false" multiValued="true" required="false" default="__nosecurity__"/>

    <dynamicField name="*_i"  type="int"    indexed="true"  stored="true"/>
    <dynamicField name="*_is" type="int"    indexed="true"  stored="true"  multiValued="true"/>
    <dynamicField name="*_s"  type="string"  indexed="true"  stored="true" />
    <dynamicField name="*_ss" type="string"  indexed="true"  stored="true" multiValued="true"/>
    <dynamicField name="*_l"  type="long"   indexed="true"  stored="true"/>
    <dynamicField name="*_ls" type="long"   indexed="true"  stored="true"  multiValued="true"/>
    <dynamicField name="*_t"  type="text_general"    indexed="true"  stored="true" />
    <dynamicField name="*_txt" type="text_general"   indexed="true"  stored="true" multiValued="true" />
    <dynamicField name="*_b"  type="boolean" indexed="true" stored="true"/>
    <dynamicField name="*_bs" type="boolean" indexed="true" stored="true"  multiValued="true"/>
    <dynamicField name="*_f"  type="float"  indexed="true"  stored="true"/>
    <dynamicField name="*_fs" type="float"  indexed="true"  stored="true"  multiValued="true"/>
    <dynamicField name="*_d"  type="double" indexed="true"  stored="true"/>
    <dynamicField name="*_ds" type="double" indexed="true"  stored="true"  multiValued="true"/>
    <dynamicField name="*_dt"  type="date"    indexed="true"  stored="true"/>
    <dynamicField name="*_dts" type="date"    indexed="true"  stored="true" multiValued="true"/>

    <!-- language specific indexed fields (term vectors on for more like this) -->
    <dynamicField name="*_de"  type="text_de" indexed="true"  stored="false" termVectors="true" />
    <dynamicField name="*_en"  type="text_en" indexed="true"  stored="false" termVectors="true" />

    <!-- some trie-coded dynamic fields for faster range queries -->
    <dynamicField name="*_ti" type="tint"    indexed="true"  stored="true"/>
    <dynamicField name="*_tl" type="tlong"   indexed="true"  stored="true"/>
    <dynamicField name="*_tf" type="tfloat"  indexed="true"  stored="true"/>
    <dynamicField name="*_td" type="tdouble" indexed="true"  stored="true"/>
    <dynamicField name="*_tdt" type="tdate"  indexed="true"  stored="true"/>

    <!-- field for path hierarchies (separator is "/") -->
    <dynamicField name="*_path" type="text_path" indexed="true" stored="false" multiValued="true"/>

    <!-- field for resource names (like filename) -->
    <dynamicField name="*_resource_name" type="resource_name" indexed="true" stored="true" />

    <dynamicField name="ignored_*" type="ignored" multiValued="true"/>

  </fields>

  <uniqueKey>id</uniqueKey>

  <types>

    <!-- The StrField type is not analyzed, but indexed/stored verbatim. -->
    <fieldType name="string" class="solr.StrField" sortMissingLast="true" />

    <!-- boolean type: "true" or "false" -->
    <fieldType name="boolean" class="solr.BoolField" sortMissingLast="true"/>

    <!--
      Default numeric field types. For faster range queries, consider the tint/tfloat/tlong/tdouble types.
    -->
    <fieldType name="int" class="solr.TrieIntField" precisionStep="0" positionIncrementGap="0"/>
    <fieldType name="float" class="solr.TrieFloatField" precisionStep="0" positionIncrementGap="0"/>
    <fieldType name="long" class="solr.TrieLongField" precisionStep="0" positionIncrementGap="0"/>
    <fieldType name="double" class="solr.TrieDoubleField" precisionStep="0" positionIncrementGap="0"/>

    <!--
     Numeric field types that index each value at various levels of precision
     to accelerate range queries when the number of values between the range
     endpoints is large. See the javadoc for NumericRangeQuery for internal
     implementation details.

     Smaller precisionStep values (specified in bits) will lead to more tokens
     indexed per value, slightly larger index size, and faster range queries.
     A precisionStep of 0 disables indexing at different precision levels.
    -->
    <fieldType name="tint" class="solr.TrieIntField" precisionStep="8" positionIncrementGap="0"/>
    <fieldType name="tfloat" class="solr.TrieFloatField" precisionStep="8" positionIncrementGap="0"/>
    <fieldType name="tlong" class="solr.TrieLongField" precisionStep="8" positionIncrementGap="0"/>
    <fieldType name="tdouble" class="solr.TrieDoubleField" precisionStep="8" positionIncrementGap="0"/>

    <fieldType name="date" class="solr.TrieDateField" precisionStep="0" positionIncrementGap="0"/>

    <!-- A Trie based date field for faster date range queries and date faceting. -->
    <fieldType name="tdate" class="solr.TrieDateField" precisionStep="6" positionIncrementGap="0"/>

    <!--Binary data type. The data should be sent/retrieved in as Base64 encoded Strings -->
    <fieldtype name="binary" class="solr.BinaryField"/>

    <fieldtype name="ignored" stored="false" indexed="false" multiValued="true" class="solr.StrField" />

    <fieldType name="text_path" class="solr.TextField" positionIncrementGap="100">
      <analyzer type="index">
        <tokenizer class="solr.PathHierarchyTokenizerFactory" delimiter="/" />
      </analyzer>
      <analyzer type="query">
        <tokenizer class="solr.KeywordTokenizerFactory"/>
      </analyzer>
    </fieldType>

    <!-- A text field that only splits on whitespace for exact matching of words -->
    <fieldType name="text_ws" class="solr.TextField" positionIncrementGap="100">
      <analyzer>
        <tokenizer class="solr.WhitespaceTokenizerFactory"/>
      </analyzer>
    </fieldType>

    <!-- A general text field that has reasonable, generic
         cross-language defaults: it tokenizes with StandardTokenizer,
     removes stop words from case-insensitive "stopwords.txt"
     (empty by default), and down cases.  At query time only, it
     also applies synonyms. -->
    <fieldType name="text_general" class="solr.TextField" positionIncrementGap="100">
      <analyzer type="index">
        <tokenizer class="solr.StandardTokenizerFactory"/>
        <filter class="solr.StopFilterFactory" ignoreCase="true" words="stopwords.txt" />
        <filter class="solr.SynonymFilterFactory" synonyms="synonyms.txt" ignoreCase="true" expand="true"/>
        <filter class="solr.LowerCaseFilterFactory"/>
      </analyzer>
      <analyzer type="query">
        <tokenizer class="solr.StandardTokenizerFactory"/>
        <filter class="solr.StopFilterFactory" ignoreCase="true" words="stopwords.txt" />
        <filter class="solr.LowerCaseFilterFactory"/>
      </analyzer>
    </fieldType>

    <!-- Specific field for the elevation component (used in solrconfig.xml) -->
    <fieldType name="text_elevation_de" class="solr.TextField" positionIncrementGap="100">
      <analyzer>
        <tokenizer class="solr.WhitespaceTokenizerFactory"/>
        <filter class="solr.LowerCaseFilterFactory"/>
        <filter class="solr.SynonymFilterFactory" synonyms="synonyms.txt" ignoreCase="true" expand="false"/>
        <filter class="solr.GermanNormalizationFilterFactory"/>
        <filter class="solr.GermanLightStemFilterFactory"/>
      </analyzer>
    </fieldType>

    <!-- Our main text field for content, title, etc.

         - PatternReplaceCharFilter protects spaces between numbers from tokenizing by the WhitespaceTokenizer
         - WhitespaceTokenizer is used to support the WordDelimiterFilter (StandardTokenizer removes delimiters)
         - HyphenatedWordsFilter is used to concat words hyphenated and broken into two lines (only needed on index time analysis)
         - Synonyms are applied and expanded during index time (it is intended to process synonyms before word delimiter filter
           to avoid synonym on word parts)
         - WordDelimiterFilter splits on all non-alphanumeric characters, case changes and alphabet => number transitions
           word and number parts and concatenations of them are indexed.
           On query time analysis word and number parts are not generated (otherwise search after wi-fi would not find wifi due to q.op=AND with mm=100%)
           Original terms are not preserved (no use case at this time)
         - LowerCaseFilter must come after WordDelimiterFilter (see case changes)
         - RemoveDuplicatesTokenFilter removed duplicate tokens at the same position (generated by synonyms or word delimiter filter)
         - GermanNormalizationFilter is used to handle german umlauts
         - GermanLightStemFilter performs the stemming (light stemming was chosen during stemming evaluation in DB Systel Search prototyping)
     -->
    <fieldType name="text_de" class="solr.TextField" positionIncrementGap="100">
      <analyzer type="index">
        <charFilter class="solr.PatternReplaceCharFilterFactory" pattern="(\G\b|\b\d+)\s+(\d+\b)" replacement="$1&#160;$2"/>
        <tokenizer class="solr.WhitespaceTokenizerFactory" />
        <filter class="solr.LengthFilterFactory" min="1" max="100" />
        <filter class="solr.HyphenatedWordsFilterFactory"/>
        <filter class="solr.SynonymFilterFactory" synonyms="synonyms.txt" ignoreCase="true" expand="true"/>
        <filter class="solr.WordDelimiterFilterFactory" generateWordParts="1" generateNumberParts="1" catenateWords="1" catenateNumbers="1" catenateAll="1" splitOnCaseChange="1" splitOnNumerics="1" preserveOriginal="0" />
        <filter class="solr.LowerCaseFilterFactory"/>
        <filter class="solr.RemoveDuplicatesTokenFilterFactory" />
        <filter class="solr.GermanNormalizationFilterFactory"/>
        <filter class="solr.GermanLightStemFilterFactory"/>
      </analyzer>
      <analyzer type="query">
        <charFilter class="solr.PatternReplaceCharFilterFactory" pattern="(\G\b|\b\d+)\s+(\d+\b)" replacement="$1&#160;$2"/>
        <tokenizer class="solr.WhitespaceTokenizerFactory" />
        <filter class="solr.LengthFilterFactory" min="1" max="100" />
        <filter class="solr.WordDelimiterFilterFactory" generateWordParts="0" generateNumberParts="0" catenateWords="1" catenateNumbers="1" catenateAll="0" splitOnCaseChange="1" splitOnNumerics="1" preserveOriginal="0" />
        <filter class="solr.LowerCaseFilterFactory"/>
        <filter class="solr.RemoveDuplicatesTokenFilterFactory" />
        <filter class="solr.GermanNormalizationFilterFactory"/>
        <filter class="solr.GermanLightStemFilterFactory"/>
      </analyzer>
    </fieldType>

    <fieldType name="text_en" class="solr.TextField" positionIncrementGap="100">
      <analyzer type="index">
        <charFilter class="solr.PatternReplaceCharFilterFactory" pattern="(\G\b|\b\d+)\s+(\d+\b)" replacement="$1&#160;$2"/>
        <tokenizer class="solr.WhitespaceTokenizerFactory" />
        <filter class="solr.LengthFilterFactory" min="1" max="100" />
        <filter class="solr.HyphenatedWordsFilterFactory"/>
        <filter class="solr.SynonymFilterFactory" synonyms="synonyms.txt" ignoreCase="true" expand="true"/>
        <filter class="solr.WordDelimiterFilterFactory" generateWordParts="1" generateNumberParts="1" catenateWords="1" catenateNumbers="1" catenateAll="1" splitOnCaseChange="1" splitOnNumerics="1" preserveOriginal="0" />
        <filter class="solr.LowerCaseFilterFactory"/>
        <filter class="solr.RemoveDuplicatesTokenFilterFactory" />
        <filter class="solr.SnowballPorterFilterFactory" language="English" />
      </analyzer>
      <analyzer type="query">
        <charFilter class="solr.PatternReplaceCharFilterFactory" pattern="(\G\b|\b\d+)\s+(\d+\b)" replacement="$1&#160;$2"/>
        <tokenizer class="solr.WhitespaceTokenizerFactory" />
        <filter class="solr.LengthFilterFactory" min="1" max="100" />
        <filter class="solr.WordDelimiterFilterFactory" generateWordParts="0" generateNumberParts="0" catenateWords="1" catenateNumbers="1" catenateAll="0" splitOnCaseChange="1" splitOnNumerics="1" preserveOriginal="0" />
        <filter class="solr.LowerCaseFilterFactory"/>
        <filter class="solr.RemoveDuplicatesTokenFilterFactory" />
        <filter class="solr.SnowballPorterFilterFactory" language="English" />
      </analyzer>
    </fieldType>

    <!--
        Used for spellchecking and autosuggestions
        Analyzer chain similar to text_de, but there are differences:
        - no stemming (would lead to strange suggestions)
        - lengthfilter starts with 2 characters because we need no 1 char suggestions
        - WDF: we dont want intra word splitting on case changes and numeric transitions
        - stop words are removed from suggestions
    -->
    <fieldType name="textSuggest" class="solr.TextField" positionIncrementGap="100" stored="false">
        <analyzer type="index">
            <charFilter class="solr.PatternReplaceCharFilterFactory" pattern="(\G\b|\b\d+)\s+(\d+\b)" replacement="$1&#160;$2"/>
            <tokenizer class="solr.WhitespaceTokenizerFactory" />
            <filter class="solr.LengthFilterFactory" min="2" max="100" />
            <filter class="solr.HyphenatedWordsFilterFactory"/>
            <filter class="solr.SynonymFilterFactory" synonyms="synonyms.txt" ignoreCase="true" expand="true"/>
            <filter class="solr.WordDelimiterFilterFactory" generateWordParts="1" generateNumberParts="1" catenateWords="1" catenateNumbers="1" catenateAll="1" splitOnCaseChange="0" splitOnNumerics="0" preserveOriginal="0" />
            <filter class="solr.LowerCaseFilterFactory"/>
            <filter class="solr.StopFilterFactory" ignoreCase="true" words="lang/stopwords_de.txt" format="snowball"/>
            <filter class="solr.StopFilterFactory" ignoreCase="true" words="lang/stopwords_en.txt" />
            <filter class="solr.RemoveDuplicatesTokenFilterFactory" />
        </analyzer>
        <analyzer type="query">
            <charFilter class="solr.PatternReplaceCharFilterFactory" pattern="(\G\b|\b\d+)\s+(\d+\b)" replacement="$1&#160;$2"/>
            <tokenizer class="solr.WhitespaceTokenizerFactory" />
            <filter class="solr.LengthFilterFactory" min="2" max="100" />
            <filter class="solr.WordDelimiterFilterFactory" generateWordParts="0" generateNumberParts="0" catenateWords="1" catenateNumbers="1" catenateAll="1" splitOnCaseChange="0" splitOnNumerics="0" preserveOriginal="0" />
            <filter class="solr.LowerCaseFilterFactory"/>
            <filter class="solr.StopFilterFactory" ignoreCase="true" words="lang/stopwords_de.txt" format="snowball" />
            <filter class="solr.StopFilterFactory" ignoreCase="true" words="lang/stopwords_en.txt" />
            <filter class="solr.RemoveDuplicatesTokenFilterFactory" />
        </analyzer>
    </fieldType>

	<fieldType name="alphaNumericSort" class="solr.TextField" sortMissingLast="true" omitNorms="true">
	  <analyzer>
	    <!-- KeywordTokenizer does no actual tokenizing, so the entire input string is preserved as a single token -->
	    <tokenizer class="solr.KeywordTokenizerFactory"/>
	    <!-- The LowerCase TokenFilter does what you expect, which can be when you want your sorting to be case insensitive -->
	    <filter class="solr.LowerCaseFilterFactory" />
	    <!-- The TrimFilter removes any leading or trailing whitespace -->
	    <filter class="solr.TrimFilterFactory" />
	    <!-- The GermanNormalizationFilter handles special German characters  -->
	    <filter class="solr.GermanNormalizationFilterFactory"/>
	    <!-- Left-pad numbers with zeroes -->
	    <filter class="solr.PatternReplaceFilterFactory" pattern="(\d+)" replacement="00000$1" replace="all" />
	    <!-- Left-trim zeroes to produce 6 digit numbers -->
	    <filter class="solr.PatternReplaceFilterFactory" pattern="0*([0-9]{6,})" replacement="$1" replace="all" />
	    <!-- Remove all but alphanumeric characters -->
	    <filter class="solr.PatternReplaceFilterFactory" pattern="([^a-zA-Z\d:])" replacement="" replace="all" />
	  </analyzer>
	</fieldType>

    <!--
        Resource name search field (e.g. filenames)

        - replacement of _ with - to enable WordDelimiterFilter to split on these
        - the different WordDelimiterFilter configuration for index/query is on purpose to match
          different delimiters.
     -->
    <fieldType name="resource_name" class="solr.TextField" positionIncrementGap="100">
      <analyzer type="index">
        <charFilter class="solr.PatternReplaceCharFilterFactory" pattern="_" replacement="-"/>
        <tokenizer class="solr.KeywordTokenizerFactory" />
        <filter class="solr.WordDelimiterFilterFactory"
                generateWordParts="1"
                generateNumberParts="1"
                catenateWords="1"
                catenateNumbers="1"
                catenateAll="1"
                preserveOriginal="1"
                />
        <filter class="solr.LowerCaseFilterFactory"/>
      </analyzer>
      <analyzer type="query">
        <charFilter class="solr.PatternReplaceCharFilterFactory" pattern="_" replacement="-"/>
        <tokenizer class="solr.KeywordTokenizerFactory" />
        <filter class="solr.WordDelimiterFilterFactory"
                generateWordParts="1"
                generateNumberParts="1"
                catenateWords="0"
                catenateNumbers="0"
                catenateAll="0"
                preserveOriginal="1"
                />
        <filter class="solr.LowerCaseFilterFactory"/>
      </analyzer>
    </fieldType>

     <fieldType name="string_reduced" class="solr.TextField" sortMissingLast="true" omitNorms="true">
          <analyzer>
              <charFilter class="solr.MappingCharFilterFactory" mapping="mapping-FoldToASCII.txt"/>
              <tokenizer class="solr.KeywordTokenizerFactory"/>
              <filter class="solr.StopFilterFactory"
                      ignoreCase="true"
                      words="stopwords_keywords.txt"
                      />
              <filter class="solr.LowerCaseFilterFactory" />
          </analyzer>
      </fieldType>
  </types>

</schema>
